{"version":3,"sources":["../src/index.ts"],"sourcesContent":["// import { Command } from 'commander'\r\nimport { writeFileSync, createReadStream } from 'fs'\r\nimport unzipper from 'unzipper'\r\nimport csv from 'csv-parser'\r\n\r\nconst isBadPart = /\\d/\r\nconst plusRegex = /\\+|_/g\r\nconst minusRegex = /-/g\r\nconst headers = ['GLOBALEVENTID', 'SQLDATE', 'MonthYear', 'Year', 'FractionDate', 'Actor1Code', 'Actor1Name', 'Actor1CountryCode', 'Actor1KnownGroupCode', 'Actor1EthnicCode', 'Actor1Religion1Code', 'Actor1Religion2Code', 'Actor1Type1Code', 'Actor1Type2Code', 'Actor1Type3Code', 'Actor2Code', 'Actor2Name', 'Actor2CountryCode', 'Actor2KnownGroupCode', 'Actor2EthnicCode', 'Actor2Religion1Code', 'Actor2Religion2Code', 'Actor2Type1Code', 'Actor2Type2Code', 'Actor2Type3Code', 'IsRootEvent', 'EventCode', 'EventBaseCode', 'EventRootCode', 'QuadClass', 'GoldsteinScale', 'NumMentions', 'NumSources', 'NumArticles', 'AvgTone', 'Actor1Geo_Type', 'Actor1Geo_FullName', 'Actor1Geo_CountryCode', 'Actor1Geo_ADM1Code', 'Actor1Geo_Lat', 'Actor1Geo_Long', 'Actor1Geo_FeatureID', 'Actor2Geo_Type', 'Actor2Geo_FullName', 'Actor2Geo_CountryCode', 'Actor2Geo_ADM1Code', 'Actor2Geo_Lat', 'Actor2Geo_Long', 'Actor2Geo_FeatureID', 'ActionGeo_Type', 'ActionGeo_FullName', 'ActionGeo_CountryCode', 'ActionGeo_ADM1Code', 'ActionGeo_Lat', 'ActionGeo_Long', 'ActionGeo_FeatureID', 'DATEADDED', 'SOURCEURL']\r\n\r\n/**\r\n *\r\n */\r\nasync function extractAndProcessZip (zipFilePath) {\r\n  const results = []\r\n\r\n  // Create a stream to read the zip file\r\n  const zipStream = createReadStream(zipFilePath)\r\n    .pipe(unzipper.Parse({ forceStream: true }))\r\n\r\n  for await (const entry of zipStream) {\r\n    const fileName = entry.path.toLowerCase()\r\n    const type = entry.type; // 'Directory' or 'File'\r\n    // const size = entry.vars.uncompressedSize; // There is also compressedSize;\r\n\r\n    if (type === 'File' && fileName.endsWith('.csv')) {\r\n      const csvResults = await processCsv(entry)\r\n      results.push(...csvResults)\r\n    } else {\r\n      entry.autodrain()\r\n    }\r\n  }\r\n\r\n  return results;\r\n}\r\n\r\n/**\r\n *\r\n */\r\nfunction addEntities (str, array) {\r\n  if (str.length > 0) {\r\n    str\r\n      .toLowerCase()\r\n      .split(',')\r\n      .forEach(entity => {\r\n        const trimmed = entity.trim()\r\n        if (array.indexOf(trimmed) === -1) {\r\n          array.push(trimmed)\r\n        }\r\n      })\r\n  }\r\n  return array\r\n}\r\n\r\n/**\r\n *\r\n */\r\nfunction processCsv (csvStream) {\r\n  return new Promise((resolve, reject) => {\r\n    const csvResults = [];\r\n    csvStream\r\n      .pipe(csv({\r\n        separator: '\\t',\r\n        headers\r\n      }))\r\n      .on('data', (data) => { \r\n        // const values = Object\r\n        let entities = addEntities(data.Actor1Geo_FullName, [])\r\n        entities = addEntities(data.Actor2Geo_FullName, entities)\r\n        entities = addEntities(data.Actor1Name, entities)\r\n        entities = addEntities(data.Actor2Name, entities)\r\n\r\n        const urlParts = data.SOURCEURL\r\n          .toLowerCase()\r\n          .replace(plusRegex, '-')\r\n          .split('/')\r\n        \r\n        const domain = urlParts[2]\r\n\r\n        let summary = ''\r\n        for (const part of urlParts) {\r\n          if (part.length > summary.length && part.indexOf('?') === -1 && part.indexOf('-') > -1) {\r\n            summary = part\r\n          }\r\n        }\r\n\r\n        summary = summary\r\n          .replace('.html', '')\r\n          .replace('.htm', '')\r\n          .replace(minusRegex, ' ')\r\n          .trim()\r\n\r\n        if (summary === '') {\r\n          const newSummary = []\r\n          for (let part of urlParts.slice(3)) {\r\n            if (part.length <= 1 || part === '*') {\r\n              continue\r\n            }\r\n\r\n            let index = part.indexOf('?')\r\n\r\n            if (index > -1) {\r\n              part = part.substring(0, index)\r\n            }\r\n\r\n            index = part.indexOf('.')\r\n\r\n            if (index > -1) {\r\n              part = part.substring(0, index)\r\n            }\r\n\r\n            if (part.length === 0) {\r\n              continue\r\n            }\r\n\r\n            if (isBadPart.test(part) === false) {\r\n              if (newSummary.indexOf(part) === -1) {\r\n                newSummary.push(part.replace(minusRegex, ' '))\r\n              }\r\n            }\r\n          }\r\n\r\n          summary = newSummary.join(' ')\r\n        }\r\n\r\n        const event = {\r\n          isRoot: parseInt(data.IsRootEvent),\r\n          year: parseInt(data.Year),\r\n          month: parseInt(data.MonthYear.substring(4)),\r\n          day: parseInt(data.SQLDATE.substring(6)),\r\n          quad: parseInt(data.QuadClass),\r\n          goldstein: parseInt(data.GoldsteinScale),\r\n          mentions: parseInt(data.NumMentions),\r\n          sources: parseInt(data.NumSources),\r\n          articles: parseInt(data.NumArticles),\r\n          tone: parseFloat(data.AvgTone),\r\n          domain,\r\n          url: data.SOURCEURL,\r\n          summary,\r\n          entities\r\n        }\r\n        \r\n        csvResults.push(event)\r\n      })\r\n      .on('end', () => resolve(csvResults))\r\n      .on('error', (error) => reject(error));\r\n  });\r\n}\r\n\r\n/**\r\n *\r\n */\r\nasync function main () {\r\n  const zipFilePath = 'data/20240512.export.CSV.zip';\r\n  const results = await extractAndProcessZip(zipFilePath)\r\n  writeFileSync('output.json', JSON.stringify(results, null, 2))\r\n}\r\n\r\nmain()\r\n\r\n/**\r\n * Get tweets piped in from a CURL query\r\n * Split words and strip stop words\r\n * Extract domains\r\n * Extact times\r\n * Extract username\r\n * INSERT username TO users IF NOT UNIQUE\r\n * INSERT tweetId, users.id, timestamp TO tweetIds IF NOT UNIQUE\r\n * INSERT word, tweetId.id TO words\r\n *   IF (word, tweetId.id) EXISTS\r\n *     iterate counter by 1\r\n *   ELSE\r\n *     set counter to 1\r\n * INSERT domain, tweetId.id TO domains\r\n *   IF (domain, tweetId.id) EXISTS\r\n *     iterate counter by 1\r\n *   ELSE\r\n *     set counter to 1\r\n * Run queries to generate report\r\n * Create a report image\r\n */\r\n"],"mappings":"2cACA,IAAAA,EAAgD,cAChDC,EAAqB,uBACrBC,EAAgB,yBAEVC,EAAY,KACZC,EAAY,QACZC,EAAa,KACbC,EAAU,CAAC,gBAAiB,UAAW,YAAa,OAAQ,eAAgB,aAAc,aAAc,oBAAqB,uBAAwB,mBAAoB,sBAAuB,sBAAuB,kBAAmB,kBAAmB,kBAAmB,aAAc,aAAc,oBAAqB,uBAAwB,mBAAoB,sBAAuB,sBAAuB,kBAAmB,kBAAmB,kBAAmB,cAAe,YAAa,gBAAiB,gBAAiB,YAAa,iBAAkB,cAAe,aAAc,cAAe,UAAW,iBAAkB,qBAAsB,wBAAyB,qBAAsB,gBAAiB,iBAAkB,sBAAuB,iBAAkB,qBAAsB,wBAAyB,qBAAsB,gBAAiB,iBAAkB,sBAAuB,iBAAkB,qBAAsB,wBAAyB,qBAAsB,gBAAiB,iBAAkB,sBAAuB,YAAa,WAAW,EAKzjC,eAAeC,EAAsBC,EAAa,CAChD,IAAMC,EAAU,CAAC,EAGXC,KAAY,oBAAiBF,CAAW,EAC3C,KAAK,EAAAG,QAAS,MAAM,CAAE,YAAa,EAAK,CAAC,CAAC,EAE7C,cAAiBC,KAASF,EAAW,CACnC,IAAMG,EAAWD,EAAM,KAAK,YAAY,EAIxC,GAHaA,EAAM,OAGN,QAAUC,EAAS,SAAS,MAAM,EAAG,CAChD,IAAMC,EAAa,MAAMC,EAAWH,CAAK,EACzCH,EAAQ,KAAK,GAAGK,CAAU,OAE1BF,EAAM,UAAU,EAIpB,OAAOH,CACT,CAKA,SAASO,EAAaC,EAAKC,EAAO,CAChC,OAAID,EAAI,OAAS,GACfA,EACG,YAAY,EACZ,MAAM,GAAG,EACT,QAAQE,GAAU,CACjB,IAAMC,EAAUD,EAAO,KAAK,EACxBD,EAAM,QAAQE,CAAO,IAAM,IAC7BF,EAAM,KAAKE,CAAO,CAEtB,CAAC,EAEEF,CACT,CAKA,SAASH,EAAYM,EAAW,CAC9B,OAAO,IAAI,QAAQ,CAACC,EAASC,IAAW,CACtC,IAAMT,EAAa,CAAC,EACpBO,EACG,QAAK,EAAAG,SAAI,CACR,UAAW,IACX,QAAAlB,CACF,CAAC,CAAC,EACD,GAAG,OAASmB,GAAS,CAEpB,IAAIC,EAAWV,EAAYS,EAAK,mBAAoB,CAAC,CAAC,EACtDC,EAAWV,EAAYS,EAAK,mBAAoBC,CAAQ,EACxDA,EAAWV,EAAYS,EAAK,WAAYC,CAAQ,EAChDA,EAAWV,EAAYS,EAAK,WAAYC,CAAQ,EAEhD,IAAMC,EAAWF,EAAK,UACnB,YAAY,EACZ,QAAQrB,EAAW,GAAG,EACtB,MAAM,GAAG,EAENwB,EAASD,EAAS,CAAC,EAErBE,EAAU,GACd,QAAWC,KAAQH,EACbG,EAAK,OAASD,EAAQ,QAAUC,EAAK,QAAQ,GAAG,IAAM,IAAMA,EAAK,QAAQ,GAAG,EAAI,KAClFD,EAAUC,GAUd,GANAD,EAAUA,EACP,QAAQ,QAAS,EAAE,EACnB,QAAQ,OAAQ,EAAE,EAClB,QAAQxB,EAAY,GAAG,EACvB,KAAK,EAEJwB,IAAY,GAAI,CAClB,IAAME,EAAa,CAAC,EACpB,QAASD,KAAQH,EAAS,MAAM,CAAC,EAAG,CAClC,GAAIG,EAAK,QAAU,GAAKA,IAAS,IAC/B,SAGF,IAAIE,EAAQF,EAAK,QAAQ,GAAG,EAExBE,EAAQ,KACVF,EAAOA,EAAK,UAAU,EAAGE,CAAK,GAGhCA,EAAQF,EAAK,QAAQ,GAAG,EAEpBE,EAAQ,KACVF,EAAOA,EAAK,UAAU,EAAGE,CAAK,GAG5BF,EAAK,SAAW,GAIhB3B,EAAU,KAAK2B,CAAI,IAAM,IACvBC,EAAW,QAAQD,CAAI,IAAM,IAC/BC,EAAW,KAAKD,EAAK,QAAQzB,EAAY,GAAG,CAAC,EAKnDwB,EAAUE,EAAW,KAAK,GAAG,EAG/B,IAAME,EAAQ,CACZ,OAAQ,SAASR,EAAK,WAAW,EACjC,KAAM,SAASA,EAAK,IAAI,EACxB,MAAO,SAASA,EAAK,UAAU,UAAU,CAAC,CAAC,EAC3C,IAAK,SAASA,EAAK,QAAQ,UAAU,CAAC,CAAC,EACvC,KAAM,SAASA,EAAK,SAAS,EAC7B,UAAW,SAASA,EAAK,cAAc,EACvC,SAAU,SAASA,EAAK,WAAW,EACnC,QAAS,SAASA,EAAK,UAAU,EACjC,SAAU,SAASA,EAAK,WAAW,EACnC,KAAM,WAAWA,EAAK,OAAO,EAC7B,OAAAG,EACA,IAAKH,EAAK,UACV,QAAAI,EACA,SAAAH,CACF,EAEAZ,EAAW,KAAKmB,CAAK,CACvB,CAAC,EACA,GAAG,MAAO,IAAMX,EAAQR,CAAU,CAAC,EACnC,GAAG,QAAUoB,GAAUX,EAAOW,CAAK,CAAC,CACzC,CAAC,CACH,CAKA,eAAeC,GAAQ,CAErB,IAAM1B,EAAU,MAAMF,EADF,8BACkC,KACtD,iBAAc,cAAe,KAAK,UAAUE,EAAS,KAAM,CAAC,CAAC,CAC/D,CAEA0B,EAAK","names":["import_fs","import_unzipper","import_csv_parser","isBadPart","plusRegex","minusRegex","headers","extractAndProcessZip","zipFilePath","results","zipStream","unzipper","entry","fileName","csvResults","processCsv","addEntities","str","array","entity","trimmed","csvStream","resolve","reject","csv","data","entities","urlParts","domain","summary","part","newSummary","index","event","error","main"]}